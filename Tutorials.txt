Docker notes

Docker commands:
    docker pull postgres:15.1-alpine           (15.1-alpine is the version of this image)
    docker images - show the images on your machine
    docker ps - shows the currently running containers and what images they running, and various other info like ports
    docker ps --filter "status=exited"  - shows stopped containers
    docker run - creates a new container from an image (with the -d option your command line will print just the container ID
                and the container will start in detached mode..without all the logs from the container..so you can use
                terminal for other things)...
                -example "docker run redis:4.0" pulls/downloads the image specified (if no version is provided it pulls
                the latest version) if not present on your machine and creates and starts a container with that image
                - you will also need to specify the port of your machine that will connect to the port of the container,
                in order to interact with the container...for this you run a command like:
                    "docker run -p6000:6379 redis", where 6000 is your machine port, and 6379 is the port of the container
                    (see/run "docker ps" to obtain the container port)
                - --name string - you can specify a container name in this way...in care you don't like the default name,
                or in cases where for example you use 2 DBs of the same type but different versions,like 2 MongoDB versions

    docker stop "container-id" - stops the container with the mentioned container id
    docker start "container-id" - starts the container with the mentioned container id
    docker rm "container-id"    - to remove a container
    docker rm -f <the-container-id> - You can stop and remove a container in a single command by adding the "force" flag to the docker rm command. For example:
    docker ps -a - lists running and stopped containers   (useful to obtain the container id of a stopped container)

    docker logs "container-id" - to see the logs from that container
    docker exec -it "container-id" /bin/bash  - get the terminal of a running container (it=interactive terminal)
                                                if you type "exit" you exit the interactive terminal of the container

                                              - if for some containers you get error because bash is not installed try
                                              the sh approach:
                "docker exec -it e2b49956287b /bin/sh"

    docker network ls - to see the autogenerated docket networks + your own networks
    docker network create mongo-network     - to create a new network with name "mongo-network"

    docker run -p 27017:27017 -d -e MONGO_INITDB_ROOT_USERNAME=mongoadmin -e MONGO_INITDB_ROOT_PASSWORD=secret --name mongodb --net mongo-network mongo
        (-p and -d are specified above...-e refers to environment variables...these are specific to each image...in
        the above command i just provide an admin and password...other are available online on the page of the docker image)
        (--net specified the docker network name...see below for more details)

    docker run -p 8081:8081 -d -e ME_CONFIG_MONGODB_ADMINUSERNAME=mongoadmin -e ME_CONFIG_MONGODB_ADMINPASSWORD=secret -e ME_CONFIG_MONGODB_SERVER=mongodb --name mongoExpress --net mongo-network  mongo-express

    docker-compose -f docker-compose.yaml up
        ("up" option means it starts all containers)
    docker-compose -f docker-compose.yaml down
        ("down" option means that it stops all containers...)
        By default, named volumes in your compose file are NOT removed when running docker compose down. If you want to
        remove the volumes, you will need to add the --volumes flag.

   docker build -t my-app:1.0 .
!!!!    we use this to build an image by using a docker file
        (we need to specify a name and version using -t option.... The last point means that the Dockerfile location is
        in the current directory...)

   docker build -t teshte/nodeapp -f node.dockerfile .
        -here, by using the -f option, you can specify the docker file name (you can use this when you don't want to use the default
        file name for the Dockerfile..you still need the dot at the end of the command if the current folder is where this
        node.dockerfile file is located)
        -also added "teshte/" because i also want to push this image to the docker registry, and for that you need this username prefix


    docker cp pgsql-DB:/var/lib/postgresql/data/pg_hba.conf .
        TO COPY THAT file from the container to the current dir
    docker cp file.ext <container>:/path/to/file.ext
        to copy from local machine to the container

Docker network
    - docker creates its isolated docker network where the containers are running in
    (for example we can have 2 containers, MongoDB and Mongo express in the same network and so they can communicate
    with eachother using just the newtwork name, without the need to configure ips/ports etc)
    - our JS application will connect from outside using localhost and the port number


Docker Compose
    Docker Compose is a tool that was developed to help define and share multi-container applications. With Compose,
    we can create a YAML file to define the services and with a single command, can spin everything up or tear it
    all down.
    The big advantage of using Compose is you can define your application stack in a file, keep it at the root of your
    project repo (it's now version controlled), and easily enable someone else to contribute to your project. Someone
    would only need to clone your repo and start the compose app. In fact, you might see quite a few projects on
    GitHub/GitLab doing exactly this now.

    - the point of this is to have the commands, that are responsible for starting containers, in a file in a structured
    way...
    - am example of such a file is in "docker-compose.yaml" from the nodejs docker app
    - when doing this, you don't need to manually create a docker network like you did in the first example..the docker
    compose handles this

    Let's look at the logs using the "docker compose logs -f" command. You'll see the logs from each of the services
    interleaved into a single stream. This is incredibly useful when you want to watch for timing-related issues.
    The -f flag "follows" the log, so will give you live output as it's generated.

    If you want to view the logs for a specific service, you can add the service name to the end of the
    logs command (for example, "docker compose logs -f app").



Docker File
    - a blueprint for building docker images
        (see an example in the nodejs app)
    - on the first line, you need to specify on which image this new image will be based on...since we are developing a
    nodejs app we will specify "node"; this means that node will be installed in our new image..which is what we want
        FROM node:13-alpine    (the 13 represents the the version of node while alpine is a small linux version)
    - next..we can see some environment variables.. usually it is best to define these outside of the docker file, so that
    you won't need to rebuild the image, in case those settings change..
    - all capital letter words from the file are keywords for the docker file...for example RUN is used to execute all sorts
    of linux commands...like mkdir

    - "COPY ./app /home/app" copies files from the HOST machine to the image in that location...so first parameter
    is source (host machine) and the second is target (image)
    - "CMD ["node", "server.js"]" this Command is always part of the docker file..this executes an entry point linux command
    so this translates to "node server.js" and this command get executed when the image is started in a container

    - in order to build an image out of the Dockerfile you need to run a command "docker build" (see section with commands)

How to push our image on a private repository on AWS
    (no notes on this because it is not of my interest right now..)
    I created an account on docker hub for the sharing of images

    docker login -u teshte                                      (in order to login to docker hub from cmd line)
    docker tag getting-started teshte/getting-started           (in order to give a new image to an image.."teshte" comes from the repo username)
    docker push teshte/getting-started                          in order to push your image to your docker repo


Running our Image on a New Instance
    https://labs.play-with-docker.com/
        you log with your docker account and basically run your image to test it out
            docker run -dp 3000:3000 teshtegetting-started

Docker volumes
    - are used for data persistance in docker (for example if you have a docker container with a DB running inside, and
    the docker container is stopped..data is lost)
    Volumes provide the ability to connect specific filesystem paths of the container back to the host machine. If a
    directory in the container is mounted, changes in that directory are also seen on the host machine. If we mount
    that same directory across container restarts, we'd see the same files.
    - how this docker volumes works ? Folder in physical host file system is mounted into the virtual file system of Docker
    so when some data is written in the filesystem from the container, that data is also replicated in the filesystem of
    the host/your machine

    - if you start your container with "docker run" command.. you would do a docker volume like this:
        docker run -v /home/mount/data:/var/lib/mysql/data
                        host path           container path
        docker run -v /var/lib/mysql/data
            in this case you specify the container path, but no host directory...still a path will be generated by docker
            on the host machine  (this is called anonymous volumes)
         docker run -v name:/var/lib/mysql/data
            - this is called a named volume (you can specify all sort of names..This is the type that one should use in
            production setup)

    - in the docker compose file from the nodejs project we have an example with a named volume, which is called mongo-data:
    "    volumes:
           - mongo-data:/data/db    "
    also you need to specify these volumes ar the end of the file like it is done in the nodejs project docker compose example:
        "volumes:
           mongo-data:
             driver: local"
    (the benefit of this approach is that you can mount a reference of the same folder on the host to more containers)

    You can create a volume with this command:
        docker volume create todo-db
    Start the to do app container, but add the -v flag to specify a volume mount. We will use the named volume and mount
    it to /etc/todos, which will capture all files created at the path.
    (By default, the to do app stores its data in a SQLite Database at /etc/todos/todo.db  ... so this is why we use this
    path in the command from below)
        docker run -dp 3000:3000 -v todo-db:/etc/todos getting-started

    A lot of people frequently ask "Where is Docker actually storing my data when I use a named volume?" If you want to
    know, you can use the following command :
        docker volume inspect todo-db               (last param is volume name)

Bind Mounts
    With bind mounts, we control the exact mountpoint on the host. We can use this to persist data, but is often used
    to provide additional data into containers. When working on an application, we can use a bind mount to mount our
    source code into the container to let it see code changes, respond, and let us see the changes right away.

    For Node-based applications, nodemon is a great tool to watch for file changes and then restart the application.
    There are equivalent tools in most other languages and frameworks.  (there has to be something like this for java
    ..not sure if/how openliberty does this)

    Starting a Dev-Mode Container¶
    To run our container to support a development workflow, we will do the following:

        1.Mount our source code into the container
        2.Install all dependencies, including the "dev" dependencies
        3. Start nodemon to watch for filesystem changes  (this is for nodejs)

    Using bind mounts is very common for local development setups. The advantage is that the dev machine doesn't need
    to have all of the build tools and environments installed. With a single docker run command, the dev environment is
    pulled and ready to go.


Multi-Container Apps
    Up to this point, we have been working with single container apps. But, we now want to add MySQL to the application
    stack. The following question often arises -
    "Where will MySQL run? Install it in the same container or run it separately?"
    In general, each container should do one thing and do it well. A few reasons:
        1.There's a good chance you'd have to scale APIs and front-ends differently than databases.
        2.Separate containers let you version and update versions in isolation.
        3.While you may use a container for the database locally, you may want to use a managed service for the database
        in production. You don't want to ship your database engine with your app then.
        4.Running multiple processes will require a process manager (the container only starts one process), which adds
         complexity to container startup/shutdown.


Image Building Best Practices

    Security scanning
        When you have built an image, it is good practice to scan it for security vulnerabilities using the
        docker scan command. Docker has partnered with Snyk to provide the vulnerability scanning service.
        For example, to scan the getting-started image you created earlier in the tutorial, you can just type
            docker scan getting-started

        The scan uses a constantly updated database of vulnerabilities, so the output you see will vary as new
        vulnerabilities are discovered

    Image Layering
        Use the docker image history command to see the layers in the getting-started image you created earlier in
        the tutorial.
            docker image history getting-started

    Layer Caching
        Now that you've seen the layering in action, there's an important lesson to learn to help decrease build times
        for your container images.

        Once a layer changes, all downstream layers have to be recreated as well (what is a layer ? see previous chapter
        :) )

        Let's look at the Dockerfile we were using one more time...

        Going back to the image history output, we see that each command in the Dockerfile becomes a new layer in the
        image. You might remember that when we made a change to the image, the yarn dependencies had to be reinstalled.
        Is there a way to fix this? It doesn't make much sense to ship around the same dependencies every time we
        build, right?

        To fix this, we need to restructure our Dockerfile to help support the caching of the dependencies. For
        Node-based applications, those dependencies are defined in the package.json file. So what if we start by
        copying only that file in first, install the dependencies, and then copy in everything else? Then, we only
        recreate the yarn dependencies if there was a change to the package.json. Make sense?

        (basically make sure to do the things that don't change often first in the Dockerfile, and the things that change
        ofter towards the end..)

        Instead of
            COPY . .
            RUN yarn install --production
        do
            COPY package.json yarn.lock ./
            RUN yarn install --production
            COPY . .

    Multi-Stage Builds - Maven/Tomcat Example

        When building Java-based applications, a JDK is needed to compile the source code to Java bytecode. However,
        that JDK isn't needed in production. You might also be using tools such as Maven or Gradle to help build the
        app. Those also aren't needed in our final image. Multi-stage builds help.

            FROM maven AS build
            WORKDIR /app
            COPY . .
            RUN mvn package

            FROM tomcat
            COPY --from=build /app/target/file.war /usr/local/tomcat/webapps

        In this example, we use one stage (called build) to perform the actual Java build with Maven. In the second
        stage (starting at FROM tomcat), we copy in files from the build stage. The final image is only the last stage
        being created (which can be overridden using the --target flag).

    Container Orchestration¶
    Running containers in production is tough. You don't want to log into a machine and simply run a docker run or
    docker compose up. Why not? Well, what happens if the containers die? How do you scale across several machines?
    Container orchestration solves this problem. Tools like Kubernetes, Swarm, Nomad, and ECS all help solve this
    problem, all in slightly different ways.


Tutorials
    http://localhost/tutorial/our-application/          (this runs when you run the docker container tutorial)
    https://openliberty.io/guides/docker.html#launching-open-liberty-in-dev-mode        docker+openliberty combo :)
    https://ellin.com/2020/08/16/using-open-liberty-with-docker/

Other links
    https://stackoverflow.com/questions/30853247/how-do-i-edit-a-file-after-i-shell-to-a-docker-container

Video tutorials
    https://www.youtube.com/watch?v=3c-iBn73dDE         Best one yet... 10/5
    https://www.youtube.com/watch?v=GGaDSAMeopo
    https://app.pluralsight.com/library/courses/getting-started-docker/table-of-contents         very weak course 1/5
    https://app.pluralsight.com/library/courses/9f98cd6c-7c9c-4e64-a491-95e9361be47f Building and Running Your First Docker App
---------------------------------------


About the app
    when you run the command "mvn liberty:devc" this is what happens:
    "The Open Liberty Maven plug-in includes a devc goal that builds a Docker image, mounts the required directories, binds
    the required ports, and then runs the application inside of a container. This development mode, known as dev mode, also listens
    for any changes in the application source code or configuration and rebuilds the image and restarts the container as necessary."

    TODO Then try to push the image to a registry and run it in a container from that online tool mentioned by telusko and nana
        problem is that either the image is too big for that tool...or there are space issues on the tool...i could howwer install
        docker on work machine and see if it runs there
    Continue from here...try to push the rest app to docker hub and run it in the playground..
        https://labs.play-with-docker.com/

    docker run --rm -p 9080:9080 -p 9443:9443 -p 7777:7777 -e WLP_DEBUG_SUSPEND=n -e WLP_DEBUG_ADDRESS=7777 -e WLP_DEBUG_REMOTE=y -v /Users/danteshte/JavaProjects/DockerExperiments/target/.libertyDevc/apps:/config/apps -v /Users/danteshte/JavaProjects/DockerExperiments/target/.libertyDevc/dropins:/config/dropins -v /Users/danteshte/JavaProjects/DockerExperiments:/devmode -v /Users/danteshte/JavaProjects/DockerExperiments/target/liberty/wlp/usr/servers/defaultServer/logs:/logs -v /Users/danteshte/.m2/repository:/devmode-maven-cache -v /Users/danteshte/JavaProjects/DockerExperiments/src/main/liberty/config/server.xml:/config/server.xml --name liberty-dev dockerexperiments-dev-mode server debug defaultServer -- --io.openliberty.tools.projectRoot=/devmode

    Explanation of the command from above                                                                        Explanation for each option
    docker run
        --rm                                                                                                        // Automatically remove the container when it exits
        -p 9080:9080 -p 9443:9443 -p 7777:7777
        -e WLP_DEBUG_SUSPEND=n -e WLP_DEBUG_ADDRESS=7777 -e WLP_DEBUG_REMOTE=y                                      // Set environment variables
        -v /Users/danteshte/JavaProjects/DockerExperiments/target/.libertyDevc/apps:/config/apps                    // Bind mount a volume
        -v /Users/danteshte/JavaProjects/DockerExperiments/target/.libertyDevc/dropins:/config/dropins              // Bind mount a volume
        -v /Users/danteshte/JavaProjects/DockerExperiments:/devmode                                                 // Bind mount a volume
        -v /Users/danteshte/JavaProjects/DockerExperiments/target/liberty/wlp/usr/servers/defaultServer/logs:/logs  // Bind mount a volume
        -v /Users/danteshte/.m2/repository:/devmode-maven-cache                                                     // Bind mount a volume
        -v /Users/danteshte/JavaProjects/DockerExperiments/src/main/liberty/config/server.xml:/config/server.xml    // Bind mount a volume
        --name liberty-dev                                                                                          // Assign a name to the container
        dockerexperiments-dev-mode                                                                                  // this is the image name that will be run in the container
        server debug defaultServer
        -- --io.openliberty.tools.projectRoot=/devmode

    Simplified command just for running the app
        docker build -t my-rest-app:1.0 .                                                   (build the image directly, so that war is present in it..see below more info)
        docker run -d -p 9080:9080 --name my-liberty-rest-app my-rest-app:1.0               (run this new image directly)

    (this is an image that runs openliberty with the rest app inside of it. I can't run the image that is created by the
    liberty:dev build, because that is a special image, that from what I saw does not contain the .war file...See the explanation from
    below "What is the deal with all those mounted volumes from the command?", especially the part "
    Dev mode only supports the application under development in the current project so to avoid application conflicts,
    dev mode removes all Dockerfile commands that copy or add a .war file.". So..because of the hot deploymend that is needed in dev mode,
    the image created by this devmode does not contain the war and so it is just an empty open liberty server..)


    What is the deal with all those mounted volumes from the command?(dan)
        Dev mode works with a temporary, modified copy of your Dockerfile to allow for hot deployment during development as detailed below.
        When dev mode starts up, it pulls the latest version of the parent image defined in the Dockerfile, builds the container image,
        then runs the container. Note that the context of the docker build command used to generate the container image is the directory
        containing the Dockerfile, unless the dockerBuildContext parameter is specified. When dev mode exits, the container is stopped
        and deleted, and the logs are preserved in the directory mentioned above.

        Hot deployment is made possible because the application is installed as a loose application WAR. This method uses a file type
        of .war.xml which is functionally equivalent to the .war file. Dev mode only supports the application under development in the
        current project so to avoid application conflicts, dev mode removes all Dockerfile commands that copy or add a .war file.

        The .war.xml file is generated in the defaultServer/apps or the defaultServer/dropins directory so these directories are
        mounted in the container. Therefore any files that the Dockerfile may have copied into these directories in the
        container image will not be accessible.